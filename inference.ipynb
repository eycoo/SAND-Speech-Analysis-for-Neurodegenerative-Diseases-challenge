{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8954fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Available GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Available GPUs: {n_gpus}\")\n",
    "if n_gpus > 0:\n",
    "    for i in range(n_gpus):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51696ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt models defined (Tiny, Small, Base, Large)\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    \"\"\"LayerNorm for channels-first (NCHW) tensors.\"\"\"\n",
    "    def __init__(self, num_channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    \"\"\"ConvNeXt Block: DWConv -> LayerNorm -> 1x1 Conv -> GELU -> 1x1 Conv -> Drop Path\"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # Depthwise conv\n",
    "        self.norm = LayerNorm2d(dim)\n",
    "        self.pwconv1 = nn.Conv2d(dim, 4 * dim, kernel_size=1)  # Pointwise/1x1 conv\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv2d(4 * dim, dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim)) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma[:, None, None] * x\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # Binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    \"\"\"ConvNeXt architecture for image classification.\n",
    "    \n",
    "    Args:\n",
    "        in_chans: Number of input image channels (default: 3)\n",
    "        num_classes: Number of classes for classification head (default: 5)\n",
    "        depths: Number of blocks at each stage (default: [3, 3, 9, 3] for Tiny)\n",
    "        dims: Feature dimensions at each stage (default: [96, 192, 384, 768] for Tiny)\n",
    "        drop_path_rate: Stochastic depth rate (default: 0.)\n",
    "        layer_scale_init_value: Init value for Layer Scale (default: 1e-6)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, num_classes=5, \n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768],\n",
    "                 drop_path_rate=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stem: 4x4 conv with stride 4\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm2d(dims[0])\n",
    "        )\n",
    "        \n",
    "        # Build stages\n",
    "        self.stages = nn.ModuleList()\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        \n",
    "        for i in range(4):\n",
    "            # Downsampling layer (except first stage)\n",
    "            if i > 0:\n",
    "                downsample = nn.Sequential(\n",
    "                    LayerNorm2d(dims[i-1]),\n",
    "                    nn.Conv2d(dims[i-1], dims[i], kernel_size=2, stride=2)\n",
    "                )\n",
    "            else:\n",
    "                downsample = nn.Identity()\n",
    "            \n",
    "            # Stage blocks\n",
    "            stage = nn.Sequential(\n",
    "                downsample,\n",
    "                *[ConvNeXtBlock(dim=dims[i], drop_path=dp_rates[cur + j],\n",
    "                               layer_scale_init_value=layer_scale_init_value)\n",
    "                  for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "        \n",
    "        # Head\n",
    "        self.norm = LayerNorm2d(dims[-1])\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for stage in self.stages:\n",
    "            x = stage(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean([-2, -1])  # Global average pooling\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def convnext_tiny(num_classes=5, drop_path_rate=0., pretrained=False):\n",
    "    \"\"\"ConvNeXt-Tiny model\"\"\"\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768],\n",
    "                     num_classes=num_classes, drop_path_rate=drop_path_rate)\n",
    "    \n",
    "    if pretrained:\n",
    "        try:\n",
    "            # Load ImageNet pretrained weights from torchvision\n",
    "            import torchvision.models as models\n",
    "            pretrained_model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "            \n",
    "            # Copy all weights except final classifier\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() \n",
    "                             if k in model_dict and 'head' not in k}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "            print(\"✓ Loaded ImageNet pretrained weights for ConvNeXt-Tiny\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not load pretrained weights: {e}\")\n",
    "            print(\"  Training from scratch...\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_small(num_classes=5, drop_path_rate=0., pretrained=False):\n",
    "    \"\"\"ConvNeXt-Small model\"\"\"\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768],\n",
    "                     num_classes=num_classes, drop_path_rate=drop_path_rate)\n",
    "    \n",
    "    if pretrained:\n",
    "        try:\n",
    "            import torchvision.models as models\n",
    "            pretrained_model = models.convnext_small(weights='IMAGENET1K_V1')\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() \n",
    "                             if k in model_dict and 'head' not in k}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "            print(\"✓ Loaded ImageNet pretrained weights for ConvNeXt-Small\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not load pretrained weights: {e}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_base(num_classes=5, drop_path_rate=0., pretrained=False):\n",
    "    \"\"\"ConvNeXt-Base model\"\"\"\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024],\n",
    "                     num_classes=num_classes, drop_path_rate=drop_path_rate)\n",
    "    \n",
    "    if pretrained:\n",
    "        try:\n",
    "            import torchvision.models as models\n",
    "            pretrained_model = models.convnext_base(weights='IMAGENET1K_V1')\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() \n",
    "                             if k in model_dict and 'head' not in k}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "            print(\"✓ Loaded ImageNet pretrained weights for ConvNeXt-Base\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not load pretrained weights: {e}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_large(num_classes=5, drop_path_rate=0., pretrained=False):\n",
    "    \"\"\"ConvNeXt-Large model\"\"\"\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536],\n",
    "                     num_classes=num_classes, drop_path_rate=drop_path_rate)\n",
    "    \n",
    "    if pretrained:\n",
    "        try:\n",
    "            import torchvision.models as models\n",
    "            pretrained_model = models.convnext_large(weights='IMAGENET1K_V1')\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() \n",
    "                             if k in model_dict and 'head' not in k}\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "            print(\"✓ Loaded ImageNet pretrained weights for ConvNeXt-Large\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not load pretrained weights: {e}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"ConvNeXt models defined (Tiny, Small, Base, Large)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87760ef",
   "metadata": {},
   "source": [
    "# Inference on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d511e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: best_convnext_model(1).pth\n",
      "Test data: SAND_Challenge_task1_test_dataset\\task1\\test\n",
      "Output: submission.csv\n"
     ]
    }
   ],
   "source": [
    "class TestConfig:\n",
    "    MODEL_PATH = Path('best_convnext_model(1).pth')\n",
    "    TEST_DATA_PATH = Path('SAND_Challenge_task1_test_dataset/task1/test')\n",
    "    OUTPUT_CSV = Path('submission.csv')\n",
    "    \n",
    "    PHONATION_TASKS = ['phonationA', 'phonationE', 'phonationI', 'phonationO', 'phonationU']\n",
    "    RHYTHM_TASKS = ['rhythmPA', 'rhythmTA', 'rhythmKA']\n",
    "    ALL_TASKS = PHONATION_TASKS + RHYTHM_TASKS\n",
    "    \n",
    "    IMAGE_SIZE = 224\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_CLASSES = 5\n",
    "\n",
    "test_cfg = TestConfig()\n",
    "print(f\"Model: {test_cfg.MODEL_PATH}\")\n",
    "print(f\"Test data: {test_cfg.TEST_DATA_PATH}\")\n",
    "print(f\"Output: {test_cfg.OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac0a009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffRes preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "SR_DIFFRES = 22050\n",
    "N_MELS_DIFFRES = 256\n",
    "N_FFT_DIFFRES = 2048\n",
    "HOP_LENGTH_DIFFRES = 256\n",
    "\n",
    "TASK_FREQ_RANGES = {\n",
    "    'phonationA': (50, 8000),\n",
    "    'phonationE': (50, 8000),\n",
    "    'phonationI': (50, 8000),\n",
    "    'phonationO': (50, 8000),\n",
    "    'phonationU': (50, 8000),\n",
    "    'rhythmPA': (50, 8000),\n",
    "    'rhythmTA': (50, 8000),\n",
    "    'rhythmKA': (50, 8000),\n",
    "}\n",
    "\n",
    "def extract_diffres_mel_spectrogram(y, sr, task_name):\n",
    "    \"\"\"Extract high temporal resolution mel-spectrogram for DiffRes method\"\"\"\n",
    "    fmin, fmax = TASK_FREQ_RANGES.get(task_name, (50, 8000))\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y, \n",
    "        sr=sr, \n",
    "        n_fft=N_FFT_DIFFRES, \n",
    "        hop_length=HOP_LENGTH_DIFFRES,\n",
    "        n_mels=N_MELS_DIFFRES,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        power=2.0\n",
    "    )\n",
    "    \n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    if np.isnan(S_db).any():\n",
    "        S_db = np.nan_to_num(S_db, nan=-80.0)\n",
    "    \n",
    "    if np.abs(S_db).max() < 1e-6:\n",
    "        S_db = np.full_like(S_db, -80.0)\n",
    "    \n",
    "    return S_db\n",
    "\n",
    "def extract_diffres_features(y, sr, task_name, include_deltas=True):\n",
    "    \"\"\"Extract DiffRes-ready features with high temporal resolution\"\"\"\n",
    "    S_db = extract_diffres_mel_spectrogram(y, sr, task_name)\n",
    "    \n",
    "    if not include_deltas:\n",
    "        return S_db\n",
    "    \n",
    "    try:\n",
    "        delta = librosa.feature.delta(S_db, order=1)\n",
    "    except:\n",
    "        delta = np.zeros_like(S_db)\n",
    "    \n",
    "    try:\n",
    "        delta2 = librosa.feature.delta(S_db, order=2)\n",
    "    except:\n",
    "        delta2 = np.zeros_like(S_db)\n",
    "    \n",
    "    enhanced = np.stack([S_db, delta, delta2], axis=0)\n",
    "    \n",
    "    return enhanced\n",
    "\n",
    "def normalize_spectrogram(spec, method='per_sample'):\n",
    "    \"\"\"\n",
    "    Normalize spectrogram for consistent appearance.\n",
    "    \"\"\"\n",
    "    if method == 'per_sample':\n",
    "        # Z-score normalization\n",
    "        mean = spec.mean()\n",
    "        std = spec.std()\n",
    "        if std > 1e-8:\n",
    "            normalized = (spec - mean) / std\n",
    "        else:\n",
    "            normalized = spec - mean\n",
    "    elif method == 'minmax':\n",
    "        # Min-max scaling\n",
    "        min_val = spec.min()\n",
    "        max_val = spec.max()\n",
    "        if max_val - min_val > 1e-8:\n",
    "            normalized = (spec - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized = np.zeros_like(spec)\n",
    "    else:\n",
    "        normalized = spec\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def extract_mel_spectrogram_diffres(audio_path, task_name):\n",
    "    \"\"\"Extract DiffRes mel spectrogram from audio file and return as PIL Image\"\"\"\n",
    "    y, sr = librosa.load(audio_path, sr=SR_DIFFRES)\n",
    "    \n",
    "    features = extract_diffres_features(y, sr, task_name, include_deltas=True)\n",
    "    \n",
    "    if features.ndim == 3:\n",
    "        for i in range(features.shape[0]):\n",
    "            features[i] = normalize_spectrogram(features[i], method='per_sample')\n",
    "    else:\n",
    "        features = normalize_spectrogram(features, method='per_sample')\n",
    "    \n",
    "    if features.ndim == 3 and features.shape[0] >= 3:\n",
    "        r = features[0]\n",
    "        g = features[1]\n",
    "        b = features[2]\n",
    "        rgb = np.stack([r, g, b], axis=-1)\n",
    "        img_array = (rgb * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array, mode='RGB')\n",
    "    elif features.ndim == 3:\n",
    "        normalized = features[0]\n",
    "        img_array = (normalized * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array, mode='L').convert('RGB')\n",
    "    else:\n",
    "        img_array = (features * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array, mode='L').convert('RGB')\n",
    "    \n",
    "    return img\n",
    "\n",
    "print(\"DiffRes preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3cbcdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_data_path, transform=None):\n",
    "        self.test_data_path = Path(test_data_path)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        for task in test_cfg.ALL_TASKS:\n",
    "            task_folder = self.test_data_path / task\n",
    "            if task_folder.exists():\n",
    "                for wav_file in task_folder.glob('*.wav'):\n",
    "                    patient_id = wav_file.stem.split('_')[0]\n",
    "                    self.samples.append({\n",
    "                        'id': patient_id,\n",
    "                        'task': task,\n",
    "                        'path': wav_file\n",
    "                    })\n",
    "        \n",
    "        self.patient_ids = sorted(list(set([s['id'] for s in self.samples])))\n",
    "        print(f\"Found {len(self.patient_ids)} unique patients\")\n",
    "        print(f\"Total audio files: {len(self.samples)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patient_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.patient_ids[idx]\n",
    "        patient_samples = [s for s in self.samples if s['id'] == patient_id]\n",
    "        \n",
    "        images = []\n",
    "        for sample in patient_samples:\n",
    "            mel_image = extract_mel_spectrogram_diffres(sample['path'], sample['task'])\n",
    "            if self.transform:\n",
    "                mel_image = self.transform(mel_image)\n",
    "            images.append(mel_image)\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        return patient_id, images\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((test_cfg.IMAGE_SIZE, test_cfg.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Test dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c84235c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best_convnext_model(1).pth\n",
      "Model ready for inference\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path, device):\n",
    "    \"\"\"Load trained model from checkpoint\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    model = convnext_base(\n",
    "        num_classes=test_cfg.NUM_CLASSES,\n",
    "        drop_path_rate=0.1,\n",
    "        pretrained=False\n",
    "    )\n",
    "    \n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'epoch' in checkpoint:\n",
    "            print(f\"Trained for {checkpoint['epoch']} epochs\")\n",
    "        if 'best_f1' in checkpoint:\n",
    "            print(f\"Best F1 score: {checkpoint['best_f1']:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model(test_cfg.MODEL_PATH, device)\n",
    "print(\"Model ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5978fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 67 unique patients\n",
      "Total audio files: 536\n",
      "Test dataset: 67 patients\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(test_cfg.TEST_DATA_PATH, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a261ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e8d06ec4c143a3837840dde9ddcf67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions completed for 67 patients\n"
     ]
    }
   ],
   "source": [
    "def predict_test_data(model, test_loader, device):\n",
    "    \"\"\"Run inference on test data\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for patient_id, images in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            images = images.squeeze(0).to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            avg_probs = probs.mean(dim=0)\n",
    "            pred_class = avg_probs.argmax().item() + 1\n",
    "            \n",
    "            predictions.append({\n",
    "                'ID': patient_id[0].replace('ID', ''),\n",
    "                'CLASS': pred_class\n",
    "            })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict_test_data(model, test_loader, device)\n",
    "print(f\"Predictions completed for {len(predictions)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564ef9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n",
      "    ID  CLASS\n",
      "0  004      2\n",
      "1  011      2\n",
      "2  014      3\n",
      "3  019      2\n",
      "4  020      3\n",
      "5  022      3\n",
      "6  031      3\n",
      "7  032      3\n",
      "8  039      2\n",
      "9  043      2\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df = submission_df.sort_values('ID').reset_index(drop=True)\n",
    "submission_df.to_csv(test_cfg.OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"Submission saved to {test_cfg.OUTPUT_CSV}\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0de5d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS\n",
      "2    39\n",
      "3    28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total predictions: 67\n"
     ]
    }
   ],
   "source": [
    "print(submission_df['CLASS'].value_counts().sort_index())\n",
    "print(f\"\\nTotal predictions: {len(submission_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
